{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models\n",
    "from torchvision.datasets import ImageFolder\n",
    "from transformers import ViTFeatureExtractor, ViTModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "\n",
    "base_dir = \"./src/data/input/parkinsons_dataset\"\n",
    "normal_dir = os.path.join(base_dir, \"normal\")\n",
    "parkinsons_dir = os.path.join(base_dir, \"parkinson\")\n",
    "\n",
    "# Create train, validation and test directories\n",
    "output_base_dir = \"./src/data/working/processed_data\"\n",
    "train_dir = os.path.join(output_base_dir, \"train\")\n",
    "val_dir = os.path.join(output_base_dir, \"val\")\n",
    "test_dir = os.path.join(output_base_dir, \"test\")\n",
    "\n",
    "for category in [\"Normal\", \"Parkinson\"]:\n",
    "    os.makedirs(os.path.join(train_dir, category), exist_ok=True)\n",
    "    os.makedirs(os.path.join(val_dir, category), exist_ok=True)\n",
    "    os.makedirs(os.path.join(test_dir, category), exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess images\n",
    "def preprocess_images(SOURCE, DEST, target_size=(224, 224)):\n",
    "    for filename in os.listdir(SOURCE):\n",
    "        img_path = os.path.join(SOURCE, filename)\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, target_size)\n",
    "            img = img / 255.0  # Normalize to [0, 1]\n",
    "            dest_img_path = os.path.join(DEST, filename)\n",
    "            cv2.imwrite(dest_img_path, img * 255)  # Save the preprocessed image\n",
    "\n",
    "\n",
    "# Helper function to copy files\n",
    "def copy_file(file_name, src_dir, dest_dir):\n",
    "    src = os.path.join(src_dir, file_name)\n",
    "    dest = os.path.join(dest_dir, file_name)\n",
    "    shutil.copyfile(src, dest)\n",
    "\n",
    "\n",
    "# Function to split data into train, validation, and test sets\n",
    "def split_data(SOURCE, TRAINING, VALIDATION, TEST, SPLIT_SIZE=0.7, VALID_SIZE=0.15, TEST_SIZE=0.15):\n",
    "    all_files = []\n",
    "    for file_name in os.listdir(SOURCE):\n",
    "        file_path = os.path.join(SOURCE, file_name)\n",
    "        if os.path.getsize(file_path) > 0:\n",
    "            all_files.append(file_name)\n",
    "        else:\n",
    "            print(f\"{file_name} is zero length, so ignoring.\")\n",
    "\n",
    "    train_set, temp_set = train_test_split(all_files, test_size=1 - SPLIT_SIZE)\n",
    "    valid_set, test_set = train_test_split(temp_set, test_size=TEST_SIZE / (TEST_SIZE + VALID_SIZE))\n",
    "\n",
    "    for file_name in train_set:\n",
    "        copy_file(file_name, SOURCE, TRAINING)\n",
    "\n",
    "    for file_name in valid_set:\n",
    "        copy_file(file_name, SOURCE, VALIDATION)\n",
    "\n",
    "    for file_name in test_set:\n",
    "        copy_file(file_name, SOURCE, TEST)\n",
    "\n",
    "\n",
    "# Visualize some images\n",
    "def visualize_images(category, source_dir, num_images=5):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i, filename in enumerate(os.listdir(source_dir)[:num_images]):\n",
    "        img_path = os.path.join(source_dir, filename)\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is not None:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            plt.subplot(1, num_images, i + 1)\n",
    "            plt.imshow(img)\n",
    "            plt.title(f\"{category} - {i + 1}\")\n",
    "            plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each category\n",
    "for category, source_dir in [(\"Normal\", normal_dir), (\"Parkinson\", parkinsons_dir)]:\n",
    "    train_dest = os.path.join(train_dir, category)\n",
    "    val_dest = os.path.join(val_dir, category)\n",
    "    test_dest = os.path.join(test_dir, category)\n",
    "\n",
    "    split_data(source_dir, train_dest, val_dest, test_dest)\n",
    "    preprocess_images(train_dest, train_dest)\n",
    "    preprocess_images(val_dest, val_dest)\n",
    "    preprocess_images(test_dest, test_dest)\n",
    "\n",
    "    # Visualize sample images\n",
    "    print(f\"Visualizing images from {category}\")\n",
    "    visualize_images(category, train_dest)\n",
    "\n",
    "print(\"Data preprocessing and splitting completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to the processed data directories\n",
    "train_dir = \"./src/data/working/processed_data/train\"\n",
    "val_dir = \"./src/data/working/processed_data/val\"\n",
    "test_dir = \"./src/data/working/processed_data/test\"\n",
    "\n",
    "# Choose device: GPU if available, else CPU.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set basic parameters.\n",
    "num_classes = 2\n",
    "batch_size = 32  # Keep it consistent\n",
    "epochs = 30\n",
    "learning_rate = 1e-4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations for PyTorch\n",
    "transform_train = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(20),  # example \"stronger\" rotation\n",
    "        transforms.ColorJitter(\n",
    "            brightness=0.2,\n",
    "            contrast=0.2,\n",
    "            saturation=0.2,\n",
    "        ),\n",
    "        transforms.RandomAffine(\n",
    "            degrees=0,\n",
    "            translate=(0.2, 0.2),\n",
    "            scale=(0.8, 1.2),\n",
    "            shear=0.2,\n",
    "        ),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform_val = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets and loaders once\n",
    "train_dataset = ImageFolder(train_dir, transform=transform_train)\n",
    "val_dataset = ImageFolder(val_dir, transform=transform_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Training with {len(train_dataset)} images in {len(train_dataset.classes)} classes\")\n",
    "print(f\"Class mapping: {train_dataset.class_to_idx}\")\n",
    "\n",
    "# Load the pre-trained EfficientNet and freeze its weights.\n",
    "efficientnet = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT).features.to(device)\n",
    "for param in efficientnet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Load the pre-trained ViT model and its feature extractor, and freeze its weights.\n",
    "vit_model = ViTModel.from_pretrained(\"google/vit-base-patch16-224\").to(device)\n",
    "vit_feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "for param in vit_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Selectively unfreeze later layers of the models for fine-tuning\n",
    "# For EfficientNet\n",
    "for i, layer in enumerate(efficientnet):\n",
    "    if i > 6:  # Unfreeze the last few layers\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "# For ViT - unfreeze the last transformer blocks\n",
    "for i, layer in enumerate(vit_model.encoder.layer):\n",
    "    if i >= 9:  # Unfreeze the last 3 transformer blocks\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "\n",
    "# Define an ensemble model.\n",
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(EnsembleModel, self).__init__()\n",
    "        self.efficientnet = efficientnet\n",
    "        self.vit = vit_model\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # Add feature fusion layers instead of direct concatenation\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(1280 + 768, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "        self.fc = nn.Linear(1024, num_classes)\n",
    "\n",
    "    def forward(self, x, vit_inputs):\n",
    "        # EfficientNet branch.\n",
    "        eff_features = self.efficientnet(x)\n",
    "        eff_features = self.pool(eff_features)\n",
    "        eff_features = torch.flatten(eff_features, start_dim=1)\n",
    "\n",
    "        # Use precomputed inputs for ViT\n",
    "        vit_outputs = self.vit(**vit_inputs).last_hidden_state[:, 0, :]\n",
    "\n",
    "        # Combine features and pass through the final layer.\n",
    "        features = torch.cat((eff_features, vit_outputs), dim=1)\n",
    "        features = self.fusion(features)\n",
    "        output = self.fc(features)\n",
    "        return output\n",
    "\n",
    "\n",
    "# Set up the model, loss function, and optimizer.\n",
    "model = EnsembleModel(num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(\n",
    "    [\n",
    "        {\"params\": model.fusion.parameters()},\n",
    "        {\"params\": model.fc.parameters()},\n",
    "        {\n",
    "            \"params\": [p for p in model.efficientnet.parameters() if p.requires_grad],\n",
    "            \"lr\": learning_rate / 10,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for p in model.vit.parameters() if p.requires_grad],\n",
    "            \"lr\": learning_rate / 10,\n",
    "        },\n",
    "    ],\n",
    "    lr=learning_rate,\n",
    "    weight_decay=1e-5,\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\", patience=3, factor=0.5)\n",
    "\n",
    "# Training loop.\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        # Preprocess for ViT before forward pass\n",
    "        pil_images = []\n",
    "        for img_tensor in images:\n",
    "            pil_images.append(transforms.ToPILImage()(img_tensor.cpu()))\n",
    "        vit_inputs = vit_feature_extractor(images=pil_images, return_tensors=\"pt\")\n",
    "        vit_inputs = {k: v.to(device) for k, v in vit_inputs.items()}\n",
    "\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images, vit_inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # Validation phase.\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            pil_images = []\n",
    "            for img_tensor in images:\n",
    "                pil_images.append(transforms.ToPILImage()(img_tensor.cpu()))\n",
    "            vit_inputs = vit_feature_extractor(images=pil_images, return_tensors=\"pt\")\n",
    "            vit_inputs = {k: v.to(device) for k, v in vit_inputs.items()}\n",
    "\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images, vit_inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_acc = correct / total * 100.0\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "    # Step the scheduler to adjust LR based on validation loss\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model.\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(\"./src/data/models\", exist_ok=True)\n",
    "\n",
    "# Save full model (architecture + weights)\n",
    "torch.save(model, \"./src/data/models/ensemble_model_full.pt\")\n",
    "\n",
    "# Save state dict separately (current approach)\n",
    "torch.save(model.state_dict(), \"./src/data/models/ensemble_model.pth\")\n",
    "\n",
    "# Also save the feature extractor for inference\n",
    "vit_feature_extractor.save_pretrained(\"./src/data/models/vit_feature_extractor\")\n",
    "\n",
    "print(\"Model saved to ./src/data/models\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
